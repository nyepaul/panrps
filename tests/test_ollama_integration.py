
import pytest
import base64
import json
from unittest.mock import patch, MagicMock
from src.app import create_app
from src.auth.models import User
from src.models.profile import Profile
from src.models.conversation import Conversation

@pytest.fixture
def app():
    app = create_app('testing')
    app.config.update({
        "LOGIN_DISABLED": True,
        "WTF_CSRF_ENABLED": False
    })
    return app

@pytest.fixture
def client(app):
    return app.test_client()

@pytest.fixture
def mock_user_profile(monkeypatch):
    user = User(id=1, username='testuser', email='test@example.com', password_hash='hash')
    profile = MagicMock(spec=Profile)
    profile.id = 1
    profile.user_id = 1
    profile.name = 'testprofile'
    profile.birth_date = "1980-01-01"
    profile.retirement_date = "2045-01-01"
    profile.data_dict = {
        'preferred_ai_provider': 'ollama',
        'api_keys': {
            'ollama_url': 'http://localhost:11434',
            'ollama_model': 'qwen:latest'
        },
        'financial': {},
        'assets': {}
    }
    
    # Mock property access if needed
    type(profile).data = MagicMock()
    
    monkeypatch.setattr("src.routes.ai_services.current_user", user)
    monkeypatch.setattr("src.models.profile.Profile.get_by_name", lambda name, user_id: profile)
    monkeypatch.setattr("src.models.conversation.Conversation.list_by_profile", lambda uid, pid: [])
    monkeypatch.setattr("src.models.conversation.Conversation.save", lambda self: None)
    
    return profile

def test_ollama_automatic_vision_switching(client, mock_user_profile):
    """Test that extraction tasks automatically switch to llama3.2-vision for Ollama."""
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        'message': {'content': '[]'}
    }
    
    with patch('requests.post', return_value=mock_response) as mock_post:
        # We don't provide llm_provider, so it should use preferred_ai_provider (ollama)
        response = client.post('/api/extract-expenses', json={
            'image': base64.b64encode(b'fake-data').decode('utf-8'),
            'mime_type': 'application/pdf',
            'profile_name': 'testprofile'
        })
        
        assert response.status_code == 200
        # Check that llama3.2-vision was used even though qwen:latest is the profile default
        args, kwargs = mock_post.call_args
        assert kwargs['json']['model'] == 'llama3.2-vision'

def test_ollama_respects_configured_vision_model(client, mock_user_profile):
    """Test that Ollama uses the configured model if it looks like a vision model."""
    mock_user_profile.data_dict['api_keys']['ollama_model'] = 'custom-vision-v1'
    
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        'message': {'content': '[]'}
    }
    
    with patch('requests.post', return_value=mock_response) as mock_post:
        response = client.post('/api/extract-expenses', json={
            'image': base64.b64encode(b'fake-data').decode('utf-8'),
            'mime_type': 'application/pdf',
            'profile_name': 'testprofile'
        })
        
        assert response.status_code == 200
        args, kwargs = mock_post.call_args
        assert kwargs['json']['model'] == 'custom-vision-v1'

def test_advisor_chat_ollama_model_override(client, mock_user_profile):
    """Test that advisor chat respects the ollama_model override from request."""
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        'message': {'content': 'Advisor response'}
    }
    
    with patch('requests.post', return_value=mock_response) as mock_post:
        # Override the profile default (qwen:latest) with llama3
        response = client.post('/api/advisor/chat', json={
            'message': 'Hello',
            'profile_name': 'testprofile',
            'ollama_model': 'llama3'
        })
        
        assert response.status_code == 200
        args, kwargs = mock_post.call_args
        assert kwargs['json']['model'] == 'llama3'

def test_ollama_no_api_key_required(client, mock_user_profile):
    """Test that Ollama doesn't trigger 'API key not configured' error."""
    # Ensure no cloud keys are present
    mock_user_profile.data_dict['api_keys'] = {
        'ollama_url': 'http://localhost:11434'
    }
    
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.json.return_value = {
        'message': {'content': '[]'}
    }
    
    with patch('requests.post', return_value=mock_response):
        # Should NOT return 400 API key missing
        response = client.post('/api/extract-income', json={
            'image': base64.b64encode(b'fake-data').decode('utf-8'),
            'mime_type': 'application/pdf',
            'profile_name': 'testprofile'
        })
        
        assert response.status_code == 200

def test_cloud_provider_still_requires_key(client, mock_user_profile):
    """Test that cloud providers still correctly require API keys."""
    # Preferred is ollama, but we explicitly request gemini without a key in profile
    mock_user_profile.data_dict['api_keys'] = {} 
    
    response = client.post('/api/extract-assets', json={
        'image': base64.b64encode(b'fake-data').decode('utf-8'),
        'mime_type': 'application/pdf',
        'profile_name': 'testprofile',
        'llm_provider': 'gemini'
    })
    
    assert response.status_code == 400
    assert 'Gemini API key not configured' in response.get_json()['error']
